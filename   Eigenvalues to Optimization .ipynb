{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Elements of Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic Elements of Optimization__\n",
    "\n",
    "An optimization problem is defined by four parts: \n",
    "\n",
    "\n",
    "    *A set of decision variables\n",
    "    *An objective function\n",
    "    *Bounds on the decision variables\n",
    "    *constraints. \n",
    "    \n",
    "    The formulation looks like this.\n",
    "    \n",
    "    Decision Variables:\n",
    "        Given x⃗ = (x1,x2,......xn)\n",
    "        \n",
    "    Objective:\n",
    "        minimize f(x⃗)\n",
    "        \n",
    "    Bounds:\n",
    "        lb_i <= x_i ub_i where i = i.....n\n",
    "        \n",
    "    Constraints:\n",
    "        g1(x⃗)<=b_1\n",
    "        g2(x⃗)<=b_2\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "        gm(x⃗)<=b_m\n",
    "   \n",
    "\n",
    "Decision variables\n",
    "\n",
    "A vector (one-dimensional array) of the decision variables whose values we can change to find an optimal solution x⃗  = (x1, x2, …, xn) n. A solution is a set of values assigned to these decision variables.\n",
    "\n",
    "\n",
    "Objective\n",
    "\n",
    "The Objective is a function, f(x⃗ ), of the decision variables. It gives a single number evaluating a solution, which the Optimizer tries to minimize or maximize, whichever you specify in the formulation. For a linear program (LP), the objective is defined by a set of coefficients or weights that apply to the decision variables. For a nonlinear program (NLP), the objective can be any expression or variable that depends on the decision variables.\n",
    "\n",
    "\n",
    "Bounds\n",
    "\n",
    "A range lbi ≤ xi ≤ ubi, i = 1..n on the decision variables, defining what values are allowed. These bounds define the set of possible solutions, called the search space. Each decision variable can have a lower bound and/or an upper bound. If not specified, the lower and upper bounds are -INF and +INF — that is, there are no bounds.\n",
    "\n",
    "\n",
    "Constraints\n",
    "\n",
    "The constraints, e.g. g1(x⃗ ) ≤ b1, are bounds on functions of the decision variables. They define which solutions are feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance of Optimization in Machine Learning__\n",
    "\n",
    "Importance of machine learning depends on the methods and techniques we use in machine learning.\n",
    "\n",
    "Optimization is very important in some methods but not in all the methods.\n",
    "\n",
    "Example: Hastie, Tibshivani and Friedman's elements of satistical learning is considered as the most important reference in machine learning and it has very little reference to optimization.\n",
    "\n",
    "But when it comes to developing efficient and scalable algorithms to solve the machine learning problems, then optimizatoin is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Loss function or cost function is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event.\n",
    "\n",
    "A loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they’re pretty good, it’ll output a lower number. As you change pieces of your algorithm to try and improve your model, your loss function will tell you if you’re getting anywhere.\n",
    "\n",
    "In fact, we can design our own (very) basic loss function to further explain how it works. For each prediction that we make, our loss function will simply measure the absolute difference between our prediction and the actual value. In mathematical notation, it might look something like abs(y_predicted – y).\n",
    "\n",
    "Example:\n",
    "\n",
    "Here’s what some situations might look like if we were trying to predict how expensive the rent is in some NYC apartments:\n",
    "Our Predictions:       \n",
    "Harlem:$1000\n",
    "\n",
    "Salto:$2000\n",
    "\n",
    "West:$3000\n",
    "________________________________________\n",
    "\n",
    "Harlem:$500\n",
    "\n",
    "Salto:$2000\n",
    "\n",
    "West:$3000\n",
    "________________________________________\n",
    "\n",
    "Harlem:$500\n",
    "\n",
    "Salto:$1500\n",
    "\n",
    "West:$4000\n",
    "_________________________________________\n",
    "\n",
    "\n",
    "Actual Predictions:\n",
    "Harlem:$1000\n",
    "\n",
    "Salto:$2000\n",
    "\n",
    "West:$3000\n",
    "__________________________________________\n",
    "\n",
    "Our Total Loss:\n",
    "\n",
    "we dont loose any amount in first case\n",
    "\n",
    "We loose 500 in Harlem \n",
    "\n",
    "we loose 500 in Harlem and 500 in salto and have incresed the price in west\n",
    "\n",
    "\n",
    "It doesn’t matter if our predictions were too high or too low. All that matters is how incorrect we were, directionally agnostic. This is not a feature of all loss functions: in fact, your loss function will vary significantly based on the domain and unique context of the problem that you’re applying machine learning to. In your project, it may be much worse to guess too high than to guess too low, and the loss function you select must reflect that.\n",
    "\n",
    "__Loss Functions and Optimizers__\n",
    "\n",
    "Loss functions provide more than just a static representation of how your model is performing–they’re how your algorithms fit data in the first place. Most machine learning algorithms use some sort of loss function in the process of optimization, or finding the best parameters (weights) for your data.\n",
    "\n",
    "For a simple example, consider linear regression. In traditional “least squares” regression, the line of best fit is determined through none other than MSE (hence the least squares moniker)! For each set of weights that the model tries, the MSE is calculated across all input examples. The model then optimizes the MSE functions––or in other words, makes it the lowest possible––through the use of an optimizer algorithm like Gradient Descent.\n",
    "\n",
    "Just like there are different flavors of loss functions for unique problems, there is no shortage of different optimizers as well. That’s beyond the scope of this post, but in essence, the loss function and optimizer work in tandem to fit the algorithm to your data in the best way possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Operator__\n",
    "\n",
    "\n",
    "The gradient operator can be thought of as a generalization of the concept of a slope for multivariable functions. It is a vector that, given a scalar field, points to the directive of the steepest change in the function. Take for example the scalar function f(x,y,z).\n",
    "\n",
    "Its gradient is ∇f=∂f/∂xx^+∂f/∂yy^+∂f/∂zz^\n",
    "\n",
    "We can express the change in the function as df=∇f⋅dr.\n",
    "\n",
    "We see that df is maximum the dr and ∇f are parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the gradient operator~∇=∑iˆei∂/∂xi.\n",
    "\n",
    "While this looks like an ordinary vector, the coefficients arenot numbersVibut are operators, which do not commute with functions of thecoordinatesxi.\n",
    "\n",
    "We can still write out the components straightforwardly, but we must becareful to keep the order of the operators and the fields correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is Hessian related to the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Definition__\n",
    "\n",
    "Let $f:Rn→Rm$.\n",
    "\n",
    "We say, that f is differentiable at pointx0, if there exists a linear transformation A(x0), such that $f(x0+∆x) =f(x0) +A(x0)∆x+o(∆x)$\n",
    "\n",
    "We call a function  f differentiable on a set $Q⊂Rn$, \n",
    "\n",
    "if it is differentiable at each point of Q. If  f  is differentiable on Rn, we just say that  f  is differentiable. \n",
    "\n",
    "\n",
    "The matrix A(x0)  is referred to as the  derivative  or  Jacobi matrix of   f(at pointx0),\n",
    "and is denoted by $∂f(x0)∂xorf′(x0)$.\n",
    "\n",
    "Exercise:What doeso(∆x) mean in the previous definition and why isn’t ito(‖∆x‖)?\n",
    "\n",
    "Theorem 1.1$(A(x0))ij=∂fi(x0)∂xj$\n",
    "\n",
    "Exercise:\n",
    "\n",
    " First consider $f:Rn→R$ and use the total differential formula $df(x) =∑ni=1∂f∂xidxi$\n",
    " \n",
    "Theorem 1.2\n",
    "\n",
    "$(f(g(x))′=f′(g(x))g′(x)$\n",
    "\n",
    "Exercise:\n",
    "\n",
    "First consider $f:Rn→R$ and use the chain rule $∂f◦g∂xj(x) =∑ni=1∂f∂xi(g(x))·∂gi∂xj(x)$.\n",
    "\n",
    "Let $f:Rn→R$ be differentiable. Its derivative (at pointx0) is then a $1×nmatrixA(x0) =(∂f(x0)∂x1∂f(x0)∂x2...∂f(x0)∂xn)$\n",
    "\n",
    "The vectorA(x0)Tis referred to as the gradient of  f and is denoted as grad  $xf(x0) or∇xf(x0)$\n",
    "or simply $∇f(x0)$.\n",
    "\n",
    "Theorem 1.3 \n",
    "\n",
    "$∇(f(x)g(x)) =∇f(x)g(x) +f(x)∇g(x)∇(f(x)/g(x)) = (∇f(x)g(x)−f(x)∇g(x))/g2(x)∇(f(g(x))) =∇f(g(x))∇g(x) =f′(g(x))∇g(x)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "__Hessian Definition__\n",
    "\n",
    "Let $f:Rn→R$ be differentiable and $∇f:Rn→Rn$  differentiable (i.e. let $f$ be twice differentiable). \n",
    "\n",
    "The derivative of$∇f$ at point$x0$ is $ann×nmatrixH(x0)$\n",
    "\n",
    "which is referred to as the second derivativeor Hessian of $f$(at pointx0) and is denoted  as \n",
    "\n",
    "$∂2f(x0)∂2xor∇2f(x0)$.\n",
    "\n",
    "Theorem 2.1\n",
    "\n",
    "$(H(x0))ij=∂2f(x0)∂xi∂xj$\n",
    "\n",
    "Exercise: Statement follows from (1.1).\n",
    "\n",
    "\n",
    "Theorem 2.2\n",
    "\n",
    "If partial derivatives $∂2f∂xi∂xjand∂2f∂xj∂xi$  of function $f$ are continuous at $x0$, then they are equal.\n",
    "\n",
    "Thus, Hessian of a sufficiently smooth function is a symmetric matrix.\n",
    "\n",
    "Theorem 2.3\n",
    "\n",
    "All eigenvalues of a symmetric matrix are real.\n",
    "\n",
    "Theorem 2.4\n",
    "\n",
    "The set of eigenvectors of a symmetric matrix contains an orthonormalbasis as a subset.\n",
    "\n",
    "\n",
    "Theorem 2.5\n",
    "\n",
    "$f(x0+∆x) =f(x0) +∂f(x0)∂x∆x+12∆xT(∂2f(x0)∂2x)∆x+o(‖∆x‖2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The relation between the Hessian and optimization__\n",
    "\n",
    "The Hessian determines the curve of the loss function, so probably has some relation to how gradient descent would proceed. The key intuition is the relation between the eigenvalue, the eigenvector, and the speed and direction of convergence when using gradient descent. Long story short, the larger the eigenvalue, the faster the convergence from the direction of its corresponding eigenvector.\n",
    "\n",
    "Mathematically, this can be (casually) derived as follows:\n",
    "\n",
    "1. A gradient descent step with step size $\\alpha$ can be expressed as follows:\n",
    "\n",
    "$x^{(t+1)} = x^t - \\alpha (Ax^{(t)} -b)$\n",
    "\n",
    "2. If $x^{(t)} = v_i$ (the i -th eigenvector),\n",
    "\n",
    "$x^{(t+1)} = (1 - \\alpha\\lambda_i)^tx^0$\n",
    "\n",
    "where x_0 is the initial vector.\n",
    "\n",
    "3. Any vector x can be expressed as a weighted sum of eigenvectors. Therefore, for each component,  the convergence rate is $1 - \\alpha\\lambda_i$ .\n",
    "\n",
    "Intuitively, this is because the curve is steeper in directions with larger eigenvalues. Imagine a ball rolling down the loss curve. Going back to our one-dimensional example,\n",
    "\n",
    "you can see that the stronger the curve, the quicker the ball reaches the minima, where the gradient is zero. You could also imagine how fast the gradient is converging to zero. Let’s take the gradient of the two loss functions above and plot them.\n",
    "\n",
    "\n",
    "An astute reader will probably have realized, but there is no guarantee that the convergence rate $1 - \\alpha\\lambda_i $is between 0 and 1. In fact, if we choose the wrong step size $\\alpha$ , the parameters could start diverging. This is similar to how rolling a ball too quickly down the loss curve would cause it to fly off into the distance.\n",
    "\n",
    "The reason why a wrong step size could cause problems is that if the gradient changes rapidly, the gradient a few steps away would probably be completely different from its current value. We don’t necessarily mind the gradient becoming slightly different. What concerns us is whether the sign of the gradient is the same. If we take a step that is too large, we could end up actually increasing the loss. The ideal step size for each eigenvector component is when x converges to 0 in one step. Therefore, it is $\\alpha = \\frac{1}{\\lambda_i}$ .\n",
    "\n",
    "If all the eigenvalues are the same, choosing the step-size is trivial. The problem is when the eigenvalues are very different. In this case, we have to be careful not to allow any of the components to diverge: this means that the step-size is effectively bounded by the largest curvature = largest eigenvalue. On the other hand, the speed of convergence is determined by the smallest eigenvalue. This is why the ratio between the largest and smallest eigenvalues in the Hessian is a very important value and has its own name: the “condition number”. The problem of the condition number being very large is called “ill-conditioning”, and is a prevalent problem in many areas of optimization.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Eigenvectors and Eigenvalues of the Hessian__\n",
    "\n",
    "Neural networks are far too complex to analyze, so we will build our intuition using the following, simple problem: minimizing a quadratic form. Quadratic forms are nothing to be afraid of; a two-dimensional quadratic form is simply any function that can be expressed like:\n",
    "\n",
    "$\\frac{1}{2}a_{11}x_1^2 + a_{12}x_1x_2 +\\frac{1}{2}a_{22}x_2^2 + b_1x_1 + b_2x_2 + c$\n",
    "\n",
    "When we write this in matrix form, we get the following equation\n",
    "\n",
    "$\\frac{1}{2}x^TAx - b^Tx + c$\n",
    "\n",
    "where A is symmetric (its i -th row and i -th column are the same) 1. Note that this can be extended easily to arbitrarily large dimensions, and we are just focusing on the two-dimensional case for clarity.\n",
    "\n",
    "For this function, the gradient is Ax - b and the Hessian is A .  If you know matrix calculus, this is easy to derive. Otherwise, you could try deriving the derivative for each element in x.  The optimal solution is where the gradient Ax - b becomes 0. Therefore, it is $x = A^{-1}b.$\n",
    "\n",
    "The quadratic form is key to understanding the Hessian and appears all over the place, so is worth looking into in depth. \n",
    "\n",
    "The curvature in the direction of the eigenvector is determined by the eigenvalue. If the eigenvalue is larger, there is a larger curvature, and if it positive, the curvature will be positive, and vice-versa.\n",
    "\n",
    "Let’s dissect these properties one by one.\n",
    "\n",
    "Remember the definition of the Hessian? (I’ll repost it here so you don’t have to scroll up).\n",
    "\n",
    "${\\mathbf H}={\\begin{bmatrix}{\\dfrac {\\partial ^{2}f}{\\partial x_{1}^{2}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{1}\\,\\partial x_{n}}}\\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{2}^{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{2}\\,\\partial x_{n}}}\\\\[2.2ex]\\vdots &\\vdots &\\ddots &\\vdots \\\\[2.2ex]{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{1}}}&{\\dfrac {\\partial ^{2}f}{\\partial x_{n}\\,\\partial x_{2}}}&\\cdots &{\\dfrac {\\partial ^{2}f}{\\partial x_{n}^{2}}}\\end{bmatrix}}$.\n",
    "\n",
    "Let’s look at the first element in the second row,$ \\frac{\\partial^2f}{\\partial x_2 \\partial x_2}$ . \n",
    "This is the rate of change in direction $x_2$ of the gradient in direction $x_1$ . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation between quadratic form, Hessian, Precision matrix, Mahalanobis distance and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "__Hessian and Optimization__\n",
    "\n",
    "A gradient descent with step size 𝛼\n",
    "\n",
    "can be expressed as:\n",
    "\n",
    "𝑥𝑡+1=𝑥𝑡−𝛼(𝐻𝑥𝑡−𝑏)\n",
    "\n",
    "Where 𝐻\n",
    "\n",
    "is the Hessian matrix\n",
    "\n",
    "In the direction of the 𝑖𝑡ℎ\n",
    "\n",
    "eigen vector,\n",
    "\n",
    "𝑥𝑡+1=(1−𝛼𝜆𝑖)𝑡𝑥0\n",
    "\n",
    "where 𝑥0\n",
    "\n",
    "is the initial vector\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Momentum__\n",
    "\n",
    "An object which is moving has momentum. The amount of momentum (p) possessed by the moving object is the product of mass (m) and velocity (v). In equation form:\n",
    "\n",
    "$$p = m • v$$\n",
    "\n",
    "An equation such as the one above can be treated as a sort of recipe for problem-solving. Knowing the numerical values of all but one of the quantities in the equations allows one to calculate the final quantity in the equation. An equation can also be treated as a statement which describes qualitatively how one variable depends upon another. Two quantities in an equation could be thought of as being either directly proportional or inversely proportional. Momentum is directly proportional to both mass and velocity. A two-fold or three-fold increase in the mass (with the velocity held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Similarly, a two-fold or three-fold increase in the velocity (with the mass held constant) will result in a two-fold or a three-fold increase in the amount of momentum possessed by the object. Thinking and reasoning proportionally about quantities allows you to predict how an alteration in one variable would effect another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
